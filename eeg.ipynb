{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce7dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pubgp\\anaconda3\\envs\\pix2pix\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Conv1d, BatchNorm1d, Dropout\n",
    "from torch_geometric.nn import ChebConv, global_mean_pool  # <--- UPGRADE: ChebConv\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = \"SMNI_CMI_TRAIN\"\n",
    "TEST_FOLDER = \"SMNI_CMI_TEST\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0005  \n",
    "CORRELATION_THRESHOLD = 0.6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b069a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_CHANNELS = [\n",
    "    'FP1', 'FP2', 'F7', 'F8', 'AF1', 'AF2', 'FZ', 'F4', 'F3', 'FC6', 'FC5', 'FC2', 'FC1', \n",
    "    'T8', 'T7', 'CZ', 'C3', 'C4', 'CP5', 'CP6', 'CP1', 'CP2', 'P3', 'P4', 'PZ', 'P8', 'P7', \n",
    "    'PO2', 'PO1', 'O2', 'O1', 'X', 'AF7', 'AF8', 'F5', 'F6', 'FT7', 'FT8', 'FPZ', 'FC4', 'FC3', \n",
    "    'C6', 'C5', 'F2', 'F1', 'TP8', 'TP7', 'AFZ', 'CP3', 'CP4', 'P5', 'P6', 'C1', 'C2', 'PO7', \n",
    "    'PO8', 'FCZ', 'POZ', 'OZ', 'P2', 'P1', 'CPZ', 'nd', 'Y'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_matrix(dataset_list):\n",
    "    \"\"\"\n",
    "    Computes the average correlation matrix across a subset of training data\n",
    "    to determine which electrodes usually talk to each other.\n",
    "    \"\"\"\n",
    "    print(\"Computing Functional Connectivity (Correlation Graph)...\")\n",
    "    num_nodes = 64\n",
    "    sum_corr = np.zeros((num_nodes, num_nodes))\n",
    "    count = 0\n",
    "    \n",
    "    sample_size = min(len(dataset_list), 50)\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "\n",
    "        data_x = dataset_list[i].x.numpy()\n",
    "        \n",
    "        corr = np.abs(np.corrcoef(data_x))\n",
    "        \n",
    "        corr = np.nan_to_num(corr)\n",
    "        sum_corr += corr\n",
    "        count += 1\n",
    "        \n",
    "    avg_corr = sum_corr / count\n",
    "    return avg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d95eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_functional_edges(dataset_list, threshold=0.5):\n",
    "    \"\"\" Creates edge_index based on correlation > threshold \"\"\"\n",
    "    avg_corr = compute_correlation_matrix(dataset_list)\n",
    "    \n",
    "    rows, cols = np.where(avg_corr > threshold)\n",
    "    \n",
    "    mask = rows != cols\n",
    "    rows = rows[mask]\n",
    "    cols = cols[mask]\n",
    "    \n",
    "    edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "    print(f\"Graph Created! Connectivity Density: {len(rows)/(64*64):.2%}\")\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfdf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    dataset = []\n",
    "    if not os.path.exists(folder_path): return []\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            if 'sensor position' not in df.columns and 'sensor pos' in df.columns:\n",
    "                df.rename(columns={'sensor pos': 'sensor position'}, inplace=True)\n",
    "\n",
    "            grouped = df.groupby('trial number')\n",
    "            for trial_num, trial_data in grouped:\n",
    "                pivot_df = trial_data.pivot_table(index='sensor position', columns='sample num', values='sensor value')\n",
    "                pivot_df = pivot_df.reindex(STANDARD_CHANNELS).fillna(0)\n",
    "                \n",
    "                if pivot_df.shape != (64, 256): continue\n",
    "                \n",
    "                subject_id = trial_data['subject identifier'].iloc[0]\n",
    "                y_label = 1 if subject_id == 'a' else 0\n",
    "                \n",
    "                x = torch.tensor(pivot_df.values, dtype=torch.float)\n",
    "                y = torch.tensor([y_label], dtype=torch.long)\n",
    "                \n",
    "                data = Data(x=x, y=y) \n",
    "                dataset.append(data)\n",
    "        except: pass\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_ChebNet(torch.nn.Module):\n",
    "    def __init__(self, num_nodes=64, num_classes=2):\n",
    "        super(EEG_ChebNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv1d(1, 16, kernel_size=10, stride=2)\n",
    "        self.bn1 = BatchNorm1d(16)\n",
    "        self.conv2 = Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = BatchNorm1d(32)\n",
    "        \n",
    "        self.flatten_size = 32 * 60 \n",
    "\n",
    "        self.cheb1 = ChebConv(self.flatten_size, 128, K=3)\n",
    "        self.cheb2 = ChebConv(128, 64, K=3)\n",
    "\n",
    "        self.fc = Linear(64, num_classes)\n",
    "        self.dropout = Dropout(p=0.5) \n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.unsqueeze(1) \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.cheb1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.cheb2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch) \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7980cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Intelligent Graph Edges...\n",
      "Computing Functional Connectivity (Correlation Graph)...\n",
      "Graph Created! Connectivity Density: 30.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pubgp\\AppData\\Local\\Temp\\ipykernel_32288\\4060110793.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
      "C:\\Users\\pubgp\\AppData\\Local\\Temp\\ipykernel_32288\\3646644657.py:20: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  train_loader = DataLoader(train_data_raw, batch_size=BATCH_SIZE, shuffle=True)\n",
      "C:\\Users\\pubgp\\AppData\\Local\\Temp\\ipykernel_32288\\3646644657.py:21: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  test_loader = DataLoader(test_data_raw, batch_size=BATCH_SIZE, shuffle=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training (ChebNet) ---\n",
      "Epoch 01 | Loss: 0.7546 | Train: 0.5791 | Test: 0.6729\n",
      "Epoch 02 | Loss: 0.5929 | Train: 0.6859 | Test: 0.7146\n",
      "Epoch 03 | Loss: 0.5481 | Train: 0.7286 | Test: 0.7167\n",
      "Epoch 04 | Loss: 0.5288 | Train: 0.7244 | Test: 0.7417\n",
      "Epoch 05 | Loss: 0.4750 | Train: 0.7799 | Test: 0.6875\n",
      "Epoch 06 | Loss: 0.4963 | Train: 0.7286 | Test: 0.7229\n",
      "Epoch 07 | Loss: 0.4084 | Train: 0.7991 | Test: 0.6917\n",
      "Epoch 08 | Loss: 0.4189 | Train: 0.7991 | Test: 0.7167\n",
      "Epoch 09 | Loss: 0.3633 | Train: 0.8397 | Test: 0.7250\n",
      "Epoch 10 | Loss: 0.3808 | Train: 0.8162 | Test: 0.7125\n",
      "Epoch 11 | Loss: 0.3478 | Train: 0.8419 | Test: 0.7125\n",
      "Epoch 12 | Loss: 0.3264 | Train: 0.8568 | Test: 0.7104\n",
      "Epoch 13 | Loss: 0.2714 | Train: 0.8846 | Test: 0.7375\n",
      "Epoch 14 | Loss: 0.2519 | Train: 0.8803 | Test: 0.6750\n",
      "Epoch 15 | Loss: 0.2422 | Train: 0.9103 | Test: 0.7333\n",
      "Epoch 16 | Loss: 0.2536 | Train: 0.8953 | Test: 0.7167\n",
      "Epoch 17 | Loss: 0.1746 | Train: 0.9316 | Test: 0.7146\n",
      "Epoch 18 | Loss: 0.1355 | Train: 0.9423 | Test: 0.7375\n",
      "Epoch 19 | Loss: 0.1187 | Train: 0.9530 | Test: 0.6937\n",
      "Epoch 20 | Loss: 0.1070 | Train: 0.9701 | Test: 0.7354\n",
      "Epoch 21 | Loss: 0.1013 | Train: 0.9615 | Test: 0.7583\n",
      "Epoch 22 | Loss: 0.0910 | Train: 0.9573 | Test: 0.7521\n",
      "Epoch 23 | Loss: 0.1044 | Train: 0.9658 | Test: 0.7375\n",
      "Epoch 24 | Loss: 0.0885 | Train: 0.9637 | Test: 0.7333\n",
      "Epoch 25 | Loss: 0.0549 | Train: 0.9872 | Test: 0.7042\n",
      "Epoch 26 | Loss: 0.0669 | Train: 0.9701 | Test: 0.7667\n",
      "Epoch 27 | Loss: 0.1208 | Train: 0.9594 | Test: 0.7500\n",
      "Epoch 28 | Loss: 0.0520 | Train: 0.9872 | Test: 0.7583\n",
      "Epoch 29 | Loss: 0.0715 | Train: 0.9722 | Test: 0.7542\n",
      "Epoch 30 | Loss: 0.0438 | Train: 0.9808 | Test: 0.7417\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load raw data\n",
    "    train_data_raw = load_data(TRAIN_FOLDER)\n",
    "    test_data_raw = load_data(TEST_FOLDER)\n",
    "    \n",
    "    if not train_data_raw: \n",
    "        print(\"No data found.\"); exit()\n",
    "\n",
    "    print(\"Generating Intelligent Graph Edges...\")\n",
    "    smart_edge_index = get_functional_edges(train_data_raw, threshold=CORRELATION_THRESHOLD)\n",
    "    \n",
    "    for d in train_data_raw: d.edge_index = smart_edge_index\n",
    "    for d in test_data_raw: d.edge_index = smart_edge_index\n",
    "\n",
    "    train_loader = DataLoader(train_data_raw, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data_raw, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EEG_ChebNet().to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"\\n--- Starting Training (ChebNet) ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0; total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "            \n",
    "        train_acc = correct/total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        v_correct = 0; v_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index, batch.batch)\n",
    "                v_correct += (out.argmax(1) == batch.y).sum().item()\n",
    "                v_total += batch.y.size(0)\n",
    "        \n",
    "        val_acc = v_correct/v_total\n",
    "        print(f\"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f} | Train: {train_acc:.4f} | Test: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2pix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
